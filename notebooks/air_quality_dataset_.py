# -*- coding: utf-8 -*-
"""air_quality_dataset..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xKy1JCiNsH8F40jhRY4nz9wwCjfAasl1

# **Project Name**    - Air Quality (multiclassification/regresion/NN)
##### **Data**            - https://www.kaggle.com/datasets/rabieelkharoua/air-quality-and-health-impact-dataset/data

HealthImpactScore: A score indicating the overall health impact based on air quality and other related factors, ranging from 0 to 100.
HealthImpactClass: Classification of the health impact based on the health impact score:

0: 'Very High' (HealthImpactScore >= 80)

1: 'High' (60 <= HealthImpactScore < 80)

2: 'Moderate' (40 <= HealthImpactScore < 60)

3: 'Low' (20 <= HealthImpactScore < 40)

4: 'Very Low' (HealthImpactScore < 20)

#Imports
"""

# Changing scikit-learn 1.6.0 due to it's issues with XGB

!pip uninstall scikit-learn
!pip install scikit-learn==1.5.0

!pip install catboost
from catboost import CatBoostClassifier
from catboost import CatBoostRegressor

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from yellowbrick import ClassBalance
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import StratifiedShuffleSplit, KFold, GridSearchCV, cross_val_score, RandomizedSearchCV, StratifiedKFold, train_test_split
import pickle

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

from time import time

import warnings

# Suppress specific FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, message=".*'force_all_finite'.*")

"""#Data Load"""

# Data loading from GitHub repository
def load_data(file_path):
    """Load the dataset from a specified file path."""
    try:
        df = pd.read_csv(file_path, index_col=0)
        df = df.reset_index(drop=True)
        return df
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

file_path = 'https://raw.githubusercontent.com/Zanyata/Air_Quality/refs/heads/main/data/air_quality_health_impact_data.csv'
df = load_data(file_path)

"""#Check classification correctness

HealthImpactClass: Classification of the health impact based on the health impact score:

0: 'Very High' (HealthImpactScore >= 80)

1: 'High' (60 <= HealthImpactScore < 80)

2: 'Moderate' (40 <= HealthImpactScore < 60)

3: 'Low' (20 <= HealthImpactScore < 40)

4: 'Very Low' (HealthImpactScore < 20)
"""

# Creating bins for comparison with existing labels
bins = [0, 20, 40, 60, 80, 100]
bin_labels = [4, 3, 2, 1, 0]
df["Bins"] = pd.cut(df["HealthImpactScore"], bins=bins, labels=bin_labels)
df["Bins"] = df["Bins"].astype('float64')

print(df.shape, df[df['HealthImpactClass']==df['Bins']].shape)

"""We see that 231 (~4%) cases are wrongly categorized and should be fixed."""

# Function for further usage - proper labels

def classification_correction(df):
  bins = [0, 20, 40, 60, 80, 100]
  bin_labels = [4, 3, 2, 1, 0]
  df["HealthImpactClass"] = pd.cut(df["HealthImpactScore"], bins=bins, labels=bin_labels)
  df["HealthImpactClass"] = df["HealthImpactClass"].astype('float64')
  return df

df = classification_correction(df)

"""#Classification
Classificaton using HealthImpactClass

##EDA
"""

target = "HealthImpactScore"
predictors = ['AQI', 'PM10', 'PM2_5', 'NO2', 'SO2', 'O3', 'Temperature', 'Humidity', 'WindSpeed']
plt.figure(figsize=(12,8))
for i, var in enumerate(df[predictors]):
  plt.subplot(3,3,i+1)
  sns.scatterplot(data=df, x=var, y=target, hue="HealthImpactClass", size="HealthImpactClass", sizes=(20, 100), legend=i == 0)
plt.tight_layout()
plt.show()

"""Health impact increases with increasing air polution. Atmospherical conditions (temperature, humidity, wind speed) doesn't seem to have a correlation with HealthImpactScore or Class."""

df = df.drop(columns="HealthImpactScore")

df.shape

df.head(5)

df.info()

df.duplicated().value_counts()

"""No duplicates, no null, only numerical values"""

df.describe()

i = ['RespiratoryCases', 'CardiovascularCases','HospitalAdmissions']

for i in df[i]:
  plt.figure(figsize=(4,3))
  df[i].plot(kind='hist', bins=15, title=i)
  plt.gca().spines[['top', 'right',]].set_visible(False)
  plt.show()

"""RespiratoryCases and CardiovascularCases have normal distribution, HospitalAdmissions are right-skewed."""

for column in df:
  if len(df[column].value_counts()) < 3:
    print(df[column].value_counts())

"""No binary features"""

# Correlation Heatmap
plt.figure(figsize=(9,6.5))
sns.heatmap(df.corr(),annot=True,linewidths=0.5,fmt='.2f',cmap="RdYlGn")

"""We can observe based on a correlation heatmap a negative correlation between dependent variable (HealthImpactClass) and AQI. Moderate negative correlation to PM10, PM2_5 and O3. The rest of features shows weak correlation to dependent variable. There isn't visible high multicollinearity between features that could negatively impact models performance.

"""

# Visualization of class distribution
visualizer = ClassBalance()
visualizer.fit(df['HealthImpactClass'])
visualizer.show()

"""Imbalanced dataset, this needs to be taken in to concideration while preparing train and test splits."""

# Outliers visualization
sc = StandardScaler()
labels=df.drop(columns="HealthImpactClass").columns
df_sc = sc.fit_transform(df.iloc[:, :-1])
plt.figure(figsize=(15,9))
sns.boxplot(data=df_sc, orient="h")
plt.yticks(ticks=range(len(labels)), labels=labels)
plt.xlabel("Standarized Values")
plt.ylabel("Features")
plt.title("Boxplot of Standarized Veatures")
plt.show()

"""There are some outliers. This data might be vital for ML interpretation so it shouldn't be removed on this preliminaty stage.

##Splitting
"""

# Splitting data in to train and test sets, based on HelthImpactClass due to high impalance
def stratify_splitting(df):
  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  try:
    for train_index, test_index in split.split(df, df["HealthImpactClass"]):
      strat_train_set = df.loc[train_index]
      strat_test_set = df.loc[test_index]
    feature_names = strat_train_set.drop(columns=["HealthImpactScore", "HealthImpactClass"]).columns
    return strat_train_set, strat_test_set, feature_names
  except Exception as e:
    print(f"Error while splitting: {e}")
    return None

# Splitting to X and y NumPy arrays
def xy_splitting_scaling(strat_train_set, strat_test_set):
  try:
    X_train = strat_train_set.drop(columns=["HealthImpactScore", "HealthImpactClass"])
    y_train = strat_train_set["HealthImpactClass"].values # Convert to NumPy array
    X_test = strat_test_set.drop(columns=["HealthImpactScore", "HealthImpactClass"])
    y_test = strat_test_set["HealthImpactClass"].values
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    return X_train, y_train, X_test, y_test
  except Exception as e:
    print(f"Error while xy_splitting: {e}")

# Dataset with corrected classes
file_path = 'https://raw.githubusercontent.com/Zanyata/Air_Quality/refs/heads/main/data/air_quality_health_impact_data.csv'
df = load_data(file_path)
df = classification_correction(df)
strat_train_set, strat_test_set, feature_names = stratify_splitting(df)
X_train, y_train, X_test, y_test = xy_splitting_scaling(strat_train_set, strat_test_set)

# Raw dataset without correcting classes - for ML performance checking
file_path = 'https://raw.githubusercontent.com/Zanyata/Air_Quality/refs/heads/main/data/air_quality_health_impact_data.csv'
df_raw = load_data(file_path)
strat_train_set_raw, strat_test_set_raw, feature_names = stratify_splitting(df_raw)
X_train_raw, y_train_raw, X_test_raw, y_test_raw = xy_splitting_scaling(strat_train_set_raw, strat_test_set_raw)

"""##ML

###Manual MultiML

Since it is a smaller dataset, the performance of all standart models can be checked.
"""

# Define models with appropriate class weight handling
models = {
    "log": LogisticRegression(solver="lbfgs", class_weight="balanced"),
    "raf": RandomForestClassifier(class_weight="balanced"),
    "svc": SVC(probability=True, class_weight="balanced"),
    "cat": CatBoostClassifier(verbose=0, auto_class_weights="Balanced"),
    "lgbm": LGBMClassifier(class_weight="balanced"),
    "xgb": XGBClassifier(),  #XGBoost does not have a direct parameter like scale_pos_weight for multi-class problems.
}

metrics = [precision_score, recall_score, f1_score]

def evaluate_model(models, X_train, y_train, X_test, y_test):
    scores = []

    """Evaluate the model's performance on train and test sets."""
    sample_weights = compute_sample_weight(class_weight="balanced", y=y_test)
    plt.figure(figsize=(10,6))

    for i, (name, model) in enumerate(models.items()):
      # Train model
      model.fit(X_train, y_train)

      # Predictions
      preds = model.predict(X_test)
      preds_proba = None

      if hasattr(model, "predict_proba"):  # Check if predict_proba is available
          preds_proba = model.predict_proba(X_test)

      model_scores = {"Model": name}
      for metric in metrics:
         model_scores[metric.__name__] = metric(y_test, preds, sample_weight=sample_weights, average='weighted', zero_division=0)
         # zero_division=0 parameter when calculating metrics (precision_score, recall_score, f1_score) to handle these cases.
         # This will replace undefined precision values with 0.0 instead of raising a warning.
      model_scores["accuracy"] = accuracy_score(y_test, preds, sample_weight=sample_weights)

      # Compute ROC-AUC for models with probabilities
      if preds_proba is not None:
          model_scores["roc_auc"] = roc_auc_score(
              y_test, preds_proba, sample_weight=sample_weights, multi_class="ovr", average="weighted"
          )
      scores.append(model_scores)

      # Confusion Matrix plot
      conf_matrix = confusion_matrix(y_test, preds, sample_weight=sample_weights)
      plt.subplot(2, 3, i + 1)
      sns.heatmap(conf_matrix,
                  annot=True,
                  fmt=".0f",
                  xticklabels=np.unique(y_test),
                  yticklabels=np.unique(y_test))
      plt.xlabel('Predicted Label')
      plt.ylabel('True Label')
      plt.title(f'Confusion Matrix {name}')
    plt.tight_layout()
    plt.show()

    results_df = pd.DataFrame(scores)

    print(results_df.to_string(index=False))

"""Initial dataset (raw) analysis, without labels correction:"""

evaluate_model(models, X_train_raw, y_train_raw, X_test_raw, y_test_raw)

"""Dataset analysis after labels correction:"""

evaluate_model(models, X_train, y_train, X_test, y_test)

"""Corrected dataset shows very high linear separablility, meaning simple linear decision boundaries work well.
Some models (Random Forest, SVC) perform worse, indicating possible influence of class imbalance.
The best models are Logistic Regression, CatBoost Classifier and XGB Classifier.

This shows clearly how much of noise those 231 (~4%) wrongly categorized samples contribute. For highly noised data as dataset analysis without correcting classification shows, CatBoost Classifier and LGBM Classifier would be a good choice.

###Hyperparameters tuning after classification correction
Logistic Regression, CatBoost Classifier, XGBoost Classifier
"""

# Define metrics to evaluate
metrics = [precision_score, recall_score, f1_score]

# Evaluation function
def evaluate(preds, preds_proba, metrics, y_test):
  # Sample_weight adjusts for class imbalance, while average="weighted" ensures a proportionally balanced contribution to the metric.
    scores = []
    sample_weights = compute_sample_weight(class_weight="balanced", y=y_test)
    for metric in metrics:
        scores.append(metric(y_test, preds, sample_weight=sample_weights, average='weighted', zero_division=0))

    # Compute ROC-AUC score
    accuracy = accuracy_score(y_test, preds, sample_weight=sample_weights)
    scores.append(accuracy)
    roc_auc = roc_auc_score(y_test, preds_proba, sample_weight=sample_weights, multi_class="ovr", average="weighted")
    scores.append(roc_auc)

    # Print classification report
    print("\n=== Classification Report ===")
    print(classification_report(y_test, preds))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, preds, sample_weight=sample_weights)
    return scores, conf_matrix

# Function to plot evaluation results
def plot_evaluation(scores, conf_matrix):
    print("\n=== Evaluation Metrics ===")
    print('Precision: '+str(scores[1]))
    print('Recall: '+str(scores[2]))
    print('F1 Score: '+str(scores[3]))
    print('Accuracy: '+str(scores[0]))
    print('ROC-AUC Score: '+str(scores[4]))

    # Plot confusion matrix with heatmap
    sns.heatmap(conf_matrix,
                  annot=True,
                  fmt=".0f",
                  xticklabels=np.unique(y_test),
                  yticklabels=np.unique(y_test))
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix')
    plt.show()

# Model training using randomized grid search for training speed-up
def model_tuning(model, param_grid, model_name, nIter):

  folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

  # RandomizedSearchCV
  GS = RandomizedSearchCV(
      estimator=model,
      param_distributions=param_grid,
      n_iter=nIter,
      scoring= 'f1_weighted',
      cv=folds,
      return_train_score=True,
      verbose=0,
      n_jobs=-1,  # Use all CPUs
      refit=True,
      random_state=42
  )

  # Model taining and computing predictions
  GS.fit(X_train, y_train)
  best_model = GS.best_estimator_
  best_params = GS.best_params_
  preds_train= GS.predict(X_train)
  preds_test = GS.predict(X_test)
  preds_proba_test = GS.predict_proba(X_test)

  sample_weights = compute_sample_weight(class_weight="balanced", y=y_train)

  train_precision = precision_score(y_train, preds_train, sample_weight=sample_weights, average="weighted", zero_division=0)
  train_recall = recall_score(y_train, preds_train, sample_weight=sample_weights, average="weighted", zero_division=0)
  train_f1 = f1_score(y_train, preds_train, sample_weight=sample_weights, average="weighted", zero_division=0)
  train_accuracy = accuracy_score(y_train, preds_train, sample_weight=sample_weights)

  print("=== Training Metrics ===")
  print(f"Train Precision: {train_precision:.4f}")
  print(f"Train Recall: {train_recall:.4f}")
  print(f"Train F1 Score: {train_f1:.4f}")
  print(f"Train Accuracy: {train_accuracy:.4f}")

  # Saving the best model
  filename = f"{model_name}.sav"
  pickle.dump(best_model, open(filename, "wb"))
  print(f"Best Parameters: {best_params}")
  return best_model, preds_test, preds_proba_test

"""###Logistic Regression"""

# Model instantiation
model = LogisticRegression()

model_name = "class_Log"

# Parameter Grid
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', None],  # Regularization type
    'C': np.logspace(-4, 4, 10),  # Regularization strength (inverse of lambda)
    'solver': ['liblinear', 'lbfgs', 'saga'],  # Optimization algorithm
    'max_iter': [100, 200, 500],  # Maximum iterations for convergence
    'class_weight': [None, 'balanced'],
    'random_state': [42],  # Fixed for reproducibility
}

nIter = 500 # Number of parameter settings to be sampled by RandomizedSearchCV

Log, preds_Log, preds_proba_Log  = model_tuning(model, param_grid, model_name, nIter)

scores_Log, conf_matrix_Log = evaluate(preds_Log, preds_proba_Log, metrics, y_test)
plot_evaluation(scores_Log, conf_matrix_Log)

"""After hyperparameters tuning Logistic Regression metrics: presicion, recall, f1_score and accuracy improved. Slight underfitting.

###XGB
"""

# Commented out IPython magic to ensure Python compatibility.
# Model instantiation
# Logical Parameters choosed for fastest logic
model = XGBClassifier(tree_method= 'hist', objective= 'multi:softmax', eval_metric= 'merror', booster= 'gbtree', seed=42)

model_name = "class_XGB"

# Time count
start = time()

# Parameter Grid for RandomizedSearchCV
param_grid = {
    # Model complexity
    'n_estimators': [200, 300],  #  tree count, low to prevent overfitting
    'max_depth': [1, 3, 6, 9],  # maximum deph of trees
    'learning_rate': [0.01, 0.2, 0.5, 0.7, 1, 1.3],
    # Variability (Randomness)
    'subsample': [0.4, 0.6, 0.8, 1],  # how many training samples (rows) are used in each boosting round
    'colsample_bytree': [0.4, 0.6, 0.8, 1],  # how many features (columns) are randomly chosen per tree
    # Regularization
    'min_child_weight': [4, 8, 12, 16],  # minimum number of samples (instances) required in a leaf node before a split is allowed, Higher values increase regularization, meaning fewer splits (simpler trees)
    'gamma': [0.3, 0.6, 0.8, 1],  # how much the loss function must improve before making a split.
    'reg_lambda': [0, 1, 3, 5, 7, 9, 12],  # L2 regularization (Ridge-like penalty)
    'reg_alpha': [0, 0.05, 0.1, 0.5, 1, 3, 5],  # L1 regularization

}

nIter = 1000 # Number of parameter settings to be sampled by RandomizedSearchCV

XGB, preds_XGB, preds_proba_XGB = model_tuning(model, param_grid, model_name, nIter)
print(
    "Search took %.2f seconds."
#     % ((time() - start))
)
scores_XGB, conf_matrix_XGB = evaluate(preds_XGB, preds_proba_XGB, metrics, y_test)
plot_evaluation(scores_XGB, conf_matrix_XGB)

"""Despite many attempts, hyperparameters tuning wasn't succesful. XGB is highly overfitted."""

# Feature importances
feature_importance = XGB.feature_importances_
plt.barh(range(len(feature_importance)), feature_importance)
plt.yticks(range(len(feature_importance)), feature_names)
plt.xlabel("Feature Importance")
plt.title("XGB Feature Importance")
plt.show()

"""The 4 most importans features are:

AQI: Air Quality Index, a measure of how polluted the air currently is or how polluted it is forecast to become.

PM10: Concentration of particulate matter less than 10 micrometers in diameter (μg/m³).

PM2_5: Concentration of particulate matter less than 2.5 micrometers in diameter (μg/m³).

O3: Concentration of ozone (ppb).

###CatBoost

*Early stopping is an important technique in both classification and regression tasks when using CatBoost and LightGBM, as it helps prevent overfitting by stopping training when the model's performance on a validation set stops improving*
"""

# Commented out IPython magic to ensure Python compatibility.
# Model instantiation - model with maximized test metrics
model = CatBoostClassifier(verbose=0, random_state=42, early_stopping_rounds=50, auto_class_weights='Balanced')

model_name = "class_CatBoost"

# Time counter
start = time()

# Parameter Grid
param_grid = {
    'iterations': [1000],  # Number of boosting iterations (trees)
    'learning_rate': [0.3, 0.4, 0.5],  # Step size shrinkage for weight updates
    'depth': [1, 3, 6],  # Depth of each tree
    'l2_leaf_reg': [0.1, 0.5, 0.8],  # L2 regularization coefficient
}

nIter = 27 # Number of parameter settings to be sampled by RandomizedSearchCV

best_CatBoost, preds_CatBoost, preds_proba_CatBoost  = model_tuning(model, param_grid, model_name, nIter)
print(
    "Search took %.2f seconds."
#     % ((time() - start))
)

scores_CatBoost, conf_matrix_CatBoost = evaluate(preds_CatBoost, preds_proba_CatBoost, metrics, y_test)
plot_evaluation(scores_CatBoost, conf_matrix_CatBoost)

# Commented out IPython magic to ensure Python compatibility.
# Model instantiation - model with attempting to prevent overfitting
model = CatBoostClassifier(bootstrap_type='Bayesian', verbose=0, random_state=42, early_stopping_rounds=30, auto_class_weights='Balanced')

model_name = "class_CatBoost3"

# Time counter
start = time()

# Parameter Grid
param_grid = {
    'iterations': [400, 500],  # Reduce to avoid excessive fitting
    'learning_rate': [0.05, 0.1, 0.15],  # Slightly lower for stability
    'depth': [2, 3],  # Lower depth to prevent overfitting
    'l2_leaf_reg': [2, 3, 5],  # Stronger regularization
    'rsm': [0.4, 0.5],  # Lower feature subsampling
    'loss_function': ['MultiClass'],  # Alternative for class imbalance
}



nIter = 100 # Number of parameter settings to be sampled by RandomizedSearchCV

best_CatBoost, preds_CatBoost, preds_proba_CatBoost  = model_tuning(model, param_grid, model_name, nIter)
print(
    "Search took %.2f seconds."
#     % ((time() - start))
)

scores_CatBoost, conf_matrix_CatBoost = evaluate(preds_CatBoost, preds_proba_CatBoost, metrics, y_test)
plot_evaluation(scores_CatBoost, conf_matrix_CatBoost)

"""CatBoost is highly overfitted.

ML Classification Conclusion

In classification for this particular dataset Logistic Regression, CatBoost Classifier and XGB Classifier had the best outcomes.

Even Logistic Regression has the best metrics scores, it is advisable to use more complex models.

Hyperparameter tuning for Catboost and XGB with good metrics and prevention of overfitting is hard to obtain.

#Regression
Regression using HealthImpactScore

##EDA
"""

# Histogram of HealthImpactScore
plt.figure(figsize=(4,3))
df['HealthImpactScore'].plot(kind='hist', bins=5, title="HealthImpactScore")
plt.show()

target = "HealthImpactScore"
predictors = ['AQI', 'PM10', 'PM2_5', 'NO2', 'SO2', 'O3', 'Temperature', 'Humidity', 'WindSpeed']
plt.figure(figsize=(12,8))
for i, var in enumerate(df[predictors]):
  plt.subplot(3,3,i+1)
  sns.scatterplot(data=df, x=var, y=target, hue="HealthImpactClass", size="HealthImpactClass", sizes=(20, 100), legend=i == 0)
plt.tight_layout()
plt.show()

df = df.drop(columns="HealthImpactClass")

# Correlation Heatmap
plt.figure(figsize=(9,6.5))
sns.heatmap(df.corr(),annot=True,linewidths=0.5,fmt='.2f',cmap="RdYlGn")

"""We can observe based on a correlation heatmap correlation between dependent variable (HealthImpactScore) and AQI. Also Moderate correlation to PM10, PM2_5, NO2 and O3. The rest of features shows weak correlation to dependent variable. There isn't visible high multicollinearity between features that could negatively impact models performance.

##Splitting
"""

# Splitting set for train and test, with strattification that will ensure good data distribution
def stratify_splitting(df):
  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

  # Define bin edges manually based on the value distribution
  bin_edges = [0, 25, 50, 75, 90, 100]
  df["HealthImpactScoreCat"] = pd.cut(df["HealthImpactScore"], bins=bin_edges, labels=[1, 2, 3, 4, 5], include_lowest=True)

  try:
    for train_index, test_index in split.split(df, df["HealthImpactScoreCat"]):
      strat_train_set = df.loc[train_index]
      strat_test_set = df.loc[test_index]
    feature_names = strat_train_set.drop(columns=["HealthImpactScore", "HealthImpactClass", "HealthImpactScoreCat"]).columns
    return strat_train_set, strat_test_set, feature_names
  except Exception as e:
    print(f"Error while splitting: {e}")
    return None

# Splitting data to X and y NumPy arays
def xy_splitting_scaling(strat_train_set, strat_test_set):
  try:
    X_train = strat_train_set.drop(columns=["HealthImpactScore", "HealthImpactClass", "HealthImpactScoreCat"])
    y_train = strat_train_set["HealthImpactScore"].values # Convert to NumPy array
    X_test = strat_test_set.drop(columns=["HealthImpactScore", "HealthImpactClass", "HealthImpactScoreCat"])
    y_test = strat_test_set["HealthImpactScore"].values
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    return X_train, y_train, X_test, y_test
  except Exception as e:
    print(f"Error while xy_splitting: {e}")

# Dataset load and splitting
file_path = 'https://raw.githubusercontent.com/Zanyata/Air_Quality/refs/heads/main/data/air_quality_health_impact_data.csv'
df = load_data(file_path)
strat_train_set, strat_test_set, feature_names = stratify_splitting(df)
X_train, y_train, X_test, y_test = xy_splitting_scaling(strat_train_set, strat_test_set)

"""##ML

###Manual MultiML
"""

models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "Polynomial Regression3": Pipeline([
        ('poly', PolynomialFeatures(degree=3)),
        ('lr', LinearRegression())]),
    "Random Forest": RandomForestRegressor(),
    "SVR": SVR(),
    "CatBoost": CatBoostRegressor(verbose=0),
    "LightGBM": LGBMRegressor(),
    "XGBoost": XGBRegressor()
}

metrics = [mean_absolute_error, rmse, r2_score]

def evaluate_model(models, X_train, y_train, X_test, y_test):
    scores = []
    """Evaluate the model's performance on train and test sets."""
    plt.figure(figsize=(12,8))

    for i, (name, model) in enumerate(models.items()):
      # Train model
      model.fit(X_train, y_train)

      # Predictions
      preds = model.predict(X_test)

      model_scores = {"Model": name}
      for metric in metrics:
         model_scores[metric.__name__] = metric(y_test, preds)
      scores.append(model_scores)

      # Scatter plot for actual vs predicted values
      plt.subplot(4, 3, i + 1)  # Adjusting for more models
      plt.scatter(y_test, preds, alpha=0.5, edgecolors='k')
      plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
      plt.xlabel('Actual Values')
      plt.ylabel('Predicted Values')
      plt.title(f'{name}: Actual vs Predicted')

    plt.tight_layout()
    plt.show()

    # Convert results to DataFrame and display
    results_df = pd.DataFrame(scores)
    print("Model Evaluation Results:")
    print(results_df.to_string(index=False))

evaluate_model(models, X_train, y_train, X_test, y_test)

"""Best models CatBoost Regressor, LGBM Regressor, Polynomial Regression deg.3

Linear, Ridge, and Lasso Regression models showed significantly higher MAE and RMSE, indicating that they are not suitable for this dataset.

SVR showed moderate performance but lagged behind boosting methods.

###Hyperparameters tuning
"""

# Model training, counting predictions and  training metrics
def model_tuning_reg(model, param_grid, model_name):

  folds = KFold(n_splits = 10, shuffle = True, random_state=42)
  metrics = [mean_absolute_error, rmse, r2_score]

  # GridSearchCV
  GS = GridSearchCV(
      estimator=model,
      param_grid=param_grid,
      scoring= 'neg_mean_squared_error', # focusing on penalizing larger errors more
      cv=folds,
      return_train_score=True,
      verbose=0,
      n_jobs=-1,  # Use all CPUs
      refit=True
  )

  # Training model
  GS.fit(X_train, y_train)
  best_model = GS.best_estimator_
  best_params = GS.best_params_
  preds_train= GS.predict(X_train)
  preds_test = GS.predict(X_test)

  # Training metrics
  scores = []
  for metric in metrics:
      scores.append(metric(y_train, preds_train))

  print("\n=== Training Metrics ===")
  print(f"Mean Absolute Error: {scores[0]:.4f}")
  print(f"RMSE: {scores[1]:.4f}")
  print(f"R2 Score: {scores[2]:.4f}")

  # Saving the best model
  filename = f"{model_name}.sav"
  pickle.dump(best_model, open(filename, "wb"))
  print(f"\nBest Parameters: {best_params}")
  return best_model, preds_test

# Test set evaluation and plotting confusion matrix
def evaluate_model_reg(preds, y_test):
    metrics = [mean_absolute_error, rmse, r2_score]
    scores = []
    """Evaluate the model's performance on train and test sets."""

    for metric in metrics:
        scores.append(metric(y_test, preds))

    print("\n=== Test Metrics ===")
    print(f"Mean Absolute Error: {scores[0]:.4f}")
    print(f"RMSE: {scores[1]:.4f}")
    print(f"R2 Score: {scores[2]:.4f}\n")

    # Scatter plot for actual vs predicted values
    plt.figure(figsize=(10,6))
    plt.scatter(y_test, preds, alpha=0.5, edgecolors='k')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title("Actual vs Predicted")
    plt.show()

"""###Polynomial Regression"""

# Commented out IPython magic to ensure Python compatibility.
# Model instantiation
model = Pipeline([
    ('poly', PolynomialFeatures(degree=3)),  # Polynomial transformation (degree 3)
    ('linreg', LinearRegression())  # Linear Regression model
])

model_name = "reg_Poly"

# Time counter
start = time()

# Parameter Grid
param_grid = {
    'poly__interaction_only': [False, True],  # Include only interaction terms or full polynomial
    'poly__include_bias': [False, True],  # Whether to include a bias column
    'poly__degree': [2, 3, 4],  # Test polynomial degrees
    'linreg__fit_intercept': [True, False],  # Whether to calculate the intercept
}


Poly, preds_Poly  = model_tuning_reg(model, param_grid, model_name)
print(
    "Search took %.2f seconds."
#     % ((time() - start))
)
evaluate_model_reg(preds_Poly, y_test)

"""###LGBM"""

# Commented out IPython magic to ensure Python compatibility.
# Model instantiation
model = LGBMRegressor(random_state=42)

model_name = "reg_LGBM"

# TIme counter
start = time()

# Parameter Grid
param_grid = {
    'n_estimators': [500],  # Number of boosting rounds (trees)
    'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinkage
    'max_depth': [-1, 20],  # Tree depth (-1 means no limit)
    'num_leaves': [30, 40],  # Maximum number of leaves per tree
    'min_child_samples': [30, 50],  # Minimum number of samples in a leaf
    'subsample': [0.6, 0.7, 0.8],  # Fraction of data used per iteration
    'colsample_bytree': [0.95, 1.0],  # Fraction of features used per iteration
    'reg_alpha': [0.05, 0.15, 0.1],  # L1 regularization
    'reg_lambda': [0.05, 0.1, 0.15]  # L2 regularization
}


LGBM_reg, preds_LGBM_reg  = model_tuning_reg(model, param_grid, model_name)
print(
    "Search took %.2f seconds."
#     % ((time() - start))
)
evaluate_model_reg(preds_LGBM_reg, y_test)

"""###CatBoost Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# Model instatiation
model = CatBoostRegressor(verbose=0, random_state=42, early_stopping_rounds=50)

model_name = "reg_CatBoost"

# Time counter
start = time()

# Parameter Grid
param_grid = {
    'iterations': [500],  # Number of boosting rounds
    'learning_rate': [0.05, 0.1, 0.15],  # Step size shrinkage
    'depth': [6, 8, 10],  # Maximum depth of the tree
    'l2_leaf_reg': [2, 3, 4],  # L2 regularization coefficient
    'subsample': [0.9, 1.0],  # Fraction of data used for training
}

Cat_reg, preds_Cat_reg  = model_tuning_reg(model, param_grid, model_name)
print(
    "Search took %.2f seconds."
#     % ((time() - start))
)
evaluate_model_reg(preds_Cat_reg, y_test)

"""The key signs of overfitting are:

* Very high training accuracy / R² (almost perfect fit).
* Large performance gap between train and test metrics (train much better than test).

LGBM (R2 GAP 0.0154) and CatBoost Regressor (R2 GAP 0.0061) shows very slight signs of overfitting on training set - it's acceptable.

CatBoost as the Final Model:

* Given its superior metrics across all evaluation criteria, it is the
best candidate for deployment.

LightGBM as an Alternative:

* If CatBoost is computationally expensive, LightGBM offers a solid trade-off between speed and performance.

Polynomial Regression:

* While it performs well, it might overfit complex data.

#NN Classification

Given the issues with matching ML algorithm to this classification problem, it's a good idea to check performance of very simple NN model.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow import metrics

import warnings
warnings.filterwarnings('ignore')

# Splitting X_train, y_train for additional validation set needed for NN
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)

# Sequential API
# Model definition
model = tf.keras.Sequential([
    tf.keras.layers.Dense(36, input_shape=(12,), activation="relu"),
    tf.keras.layers.Dense(36, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(36, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(units=len(np.unique(y_train)), activation="softmax"),
])

model.summary()
schema = tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)
schema

# Compiling Model
model.compile(optimizer="adam", metrics=['accuracy'], loss="sparse_categorical_crossentropy")

# Early Stopping definition
early_stopping = EarlyStopping(
    monitor="val_loss", # For an imbalanced classification problem, val_loss is generally
                        # better for early stopping to use val_loss than val_accuracy
    verbose=1,
    patience=15,
    restore_best_weights=True
)

# Model training
history = model.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    batch_size=32,
    epochs=100,
    callbacks=[early_stopping]
    )


# Get predictions on TRAINING set
preds_train_proba = model.predict(X_train)  # Predict probabilities
preds_train = np.argmax(preds_train_proba, axis=1)  # Convert probabilities to class labels

# Get predictions on TEST set
preds_test_proba = model.predict(X_test)  # Predict probabilities
preds_test = np.argmax(preds_test_proba, axis=1)  # Convert probabilities to class labels


model.evaluate(X_test, y_test)

model.save("my_model.h5")

# Evaluation function
def evaluate(preds, preds_proba, y, status):
    metrics = [precision_score, recall_score, f1_score]
    scores = []
    sample_weights = compute_sample_weight(class_weight="balanced", y=y)
    for metric in metrics:
        scores.append(metric(y, preds, sample_weight=sample_weights, average='weighted', zero_division=0))

    # Compute ROC-AUC score
    accuracy = accuracy_score(y, preds, sample_weight=sample_weights)
    scores.append(accuracy)
    roc_auc = roc_auc_score(y, preds_proba, sample_weight=sample_weights, multi_class="ovr", average="weighted")
    scores.append(roc_auc)

    # Print classification report
    print(f"\n=== Classification Report {status}===")
    print(classification_report(y, preds))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y, preds, sample_weight=sample_weights)

    print(f"\n=== Evaluation Metrics {status}===")
    print('Precision: '+str(scores[1]))
    print('Recall: '+str(scores[2]))
    print('F1 Score: '+str(scores[3]))
    print('Accuracy: '+str(scores[0]))
    print('ROC-AUC Score: '+str(scores[4]))

    return conf_matrix

# Function to plot evaluation results
def plot_evaluation(conf_matrix, status):


    # Plot confusion matrix with heatmap
    sns.heatmap(conf_matrix,
                  annot=True,
                  fmt=".0f",
                  xticklabels=np.unique(y_test),
                  yticklabels=np.unique(y_test))
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix {status}')
    plt.show()

conf_matrix_train = evaluate(preds_train, preds_train_proba, y_train, "train")
conf_matrix_test = evaluate(preds_test, preds_test_proba, y_test, "test")
plot_evaluation(conf_matrix_test, "test")

# Plotting models accuracy and lost during training
df = pd.DataFrame(history.history)

# list all data in history
print(history.history.keys())
print()
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""Simple NN model, 4 layers (2 hidden layers) performs better than best ML Classifiers. Logistic Regression has the similar performance, but using more complex algorithm might be more advisable. NN is the best solution for this classification task and doesn't show signs of overfitting."""